{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.training import batch_sequences_with_states\n",
    "\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from tensorflow.python.util import nest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# JoyStep/Net Code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joy Step Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class JoyStep(object):\n",
    "    def __init__(self):\n",
    "        self._cell = None\n",
    "        self._output_size = None\n",
    "        self._num_unroll = None\n",
    "        self._state_saver = None\n",
    "        self._base_cell = None\n",
    "        self._init_state = None\n",
    "        self._cost = None\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def cell(self):\n",
    "        \"\"\"\n",
    "        This property needs to be overwritten by the child class\n",
    "        and provide an implementation in terms of any subclass of JNNCell\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if not self._cell:\n",
    "            self._cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                self.base_cell, self.output_size + self.total_state_size)\n",
    "        return self._cell\n",
    "\n",
    "    @property\n",
    "    def base_cell(self):\n",
    "        \"\"\"Base cell without the output wrapper\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        \"\"\"size of output excluding joy step dimensions\n",
    "        \"\"\"\n",
    "        return self._output_size\n",
    "\n",
    "    @property\n",
    "    def total_output_size(self):\n",
    "        \"\"\"size of output of rnn including joy step dimensions\n",
    "        \"\"\"\n",
    "        return self.output_size + self.total_state_size\n",
    "\n",
    "    @property\n",
    "    def num_unroll(self):\n",
    "        return self._num_unroll\n",
    "\n",
    "    @property\n",
    "    def state_saver(self):\n",
    "        return self._state_saver\n",
    "\n",
    "    @property\n",
    "    def init_state(self):\n",
    "        if not self._init_state:\n",
    "            raise Exception(\"self._init_state needs to be defined during\"\n",
    "                            \" graph construction time, using the initial\"\n",
    "                            \" state from the state saving queue.\")\n",
    "        else:\n",
    "            return self._init_state\n",
    "\n",
    "    def build_joy_step_gradient(self, inputs, sequence_length):\n",
    "        with tf.name_scope('JNN'):\n",
    "            inputs = tf.unstack(inputs, num=self.num_unroll, axis=1)\n",
    "            outputs, final_state = tf.nn.static_state_saving_jnn(\n",
    "                cell=self.cell,\n",
    "                inputs=inputs,\n",
    "                state_saver=self.state_saver,\n",
    "                state_name=self.state_name,\n",
    "                sequence_length=sequence_length)\n",
    "\n",
    "            with tf.name_scope('joy_gradient'):\n",
    "                joy_gradient = tf.slice(\n",
    "                    outputs[0], begin=[0, self.output_size], size=[-1, -1])\n",
    "                joy_gradient = tf.split(\n",
    "                    joy_gradient, nest.flatten(self.state_size), axis=1)\n",
    "\n",
    "            with tf.name_scope('logits'):\n",
    "                stacked_outputs = tf.stack(outputs, axis=1)\n",
    "                logits = tf.slice(stacked_outputs, begin=[0, 0, 0], size=[-1, -1, self.output_size])\n",
    "\n",
    "        return logits, final_state, joy_gradient\n",
    "\n",
    "    def build_next_joy_step_gradient(self, final_state, next_inputs):\n",
    "        with tf.name_scope('next_joy_gradient'):\n",
    "            next_inputs = tf.unstack(next_inputs, num=self.num_unroll, axis=1)\n",
    "            next_output, _ = self.cell(next_inputs[0], final_state)\n",
    "            next_joy_step_gradient = tf.slice(\n",
    "                next_output, begin=[0, self.output_size], size=[-1, -1])\n",
    "            next_joy_step_gradient = tf.split(\n",
    "                next_joy_step_gradient, nest.flatten(self.state_size), axis=1)\n",
    "        return next_joy_step_gradient\n",
    "\n",
    "    @property\n",
    "    def zero_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          `list` of `Tensor` of [hidden_size] shape.\n",
    "        \"\"\"\n",
    "        init_states = self.cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "        init_states = nest.flatten(init_states)\n",
    "        init_states = tuple([tf.squeeze(state, axis=0) for state in init_states])\n",
    "        return init_states\n",
    "\n",
    "    @property\n",
    "    def init_state_dict(self):\n",
    "        return {k: v for k, v in zip(\n",
    "            nest.flatten(self.state_name), nest.flatten(self.zero_state))}\n",
    "\n",
    "    @property\n",
    "    def state_name(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          nested `tuple` of `str`\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "\n",
    "        def gen_state_name(zs):\n",
    "            nonlocal i\n",
    "            if isinstance(zs, tuple):\n",
    "                return tuple([gen_state_name(s) for s in zs])\n",
    "            else:\n",
    "                name = 'state_{}'.format(i)\n",
    "                i += 1\n",
    "                return name\n",
    "\n",
    "        return gen_state_name(self.state_size)\n",
    "\n",
    "    @property\n",
    "    def zero_initial_state_dict(self):\n",
    "        \"\"\"This property is used only for state saving queue\n",
    "        Returns:\n",
    "          `dict` where item is state_name:zero_state\n",
    "        \"\"\"\n",
    "        return {k: v for k, v in zip(\n",
    "            nest.flatten(self.state_name), nest.flatten(self.zero_state))}\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.base_cell.state_size\n",
    "\n",
    "    @property\n",
    "    def total_state_size(self):\n",
    "        state_sizes = nest.flatten(self.base_cell.state_size)\n",
    "        return sum(state_sizes)\n",
    "\n",
    "    def gradient(self, loss, tvars, next_sg, final_state):\n",
    "        grad_local = tf.gradients(ys=loss, xs=tvars, grad_ys=None,\n",
    "                                  name='local_gradients')\n",
    "        received_sg = [tf.where(self.is_done, tf.zeros_like(nsg), nsg) for nsg in next_sg]\n",
    "        grad_sg = tf.gradients(\n",
    "            ys=nest.flatten(final_state), xs=tvars, grad_ys=received_sg,\n",
    "            name='joy_step_gradients')\n",
    "        grad = [tf.add(gl, gs) if gs is not None else gl for gl, gs in zip(grad_local, grad_sg)]\n",
    "        return grad\n",
    "\n",
    "    def sg_target(self, loss, next_sg, final_state):\n",
    "        local_grad = tf.gradients(ys=loss, xs=nest.flatten(self.init_state))\n",
    "        next_sg = [tf.where(self.is_done, tf.zeros_like(grad), grad) for grad in next_sg]\n",
    "        future_grad = tf.gradients(\n",
    "            ys=nest.flatten(final_state),\n",
    "            xs=nest.flatten(self.init_state),\n",
    "            grad_ys=next_sg)\n",
    "        # for two sequence, the target is bootstrapped\n",
    "        # at the end sequence, the target is only single sequence\n",
    "        sg_target = [tf.stop_gradient(tf.add(lg, fg))\n",
    "                     for lg, fg in zip(local_grad, future_grad)]\n",
    "        return sg_target\n",
    "\n",
    "    @property\n",
    "    def is_done(self):\n",
    "        return self._is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JoyNet Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SyntheticGradientRNN(object):\n",
    "    def __init__(self):\n",
    "        self._cell = None\n",
    "        self._output_size = None\n",
    "        self._num_unroll = None\n",
    "        self._state_saver = None\n",
    "        self._base_cell = None\n",
    "        self._init_state = None\n",
    "        self._cost = None\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def cell(self):\n",
    "        \"\"\"\n",
    "        This property needs to be overwritten by the child class\n",
    "        and provide an implementation in terms of any subclass of JNNCell\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        if not self._cell:\n",
    "            self._cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "                self.base_cell, self.output_size + self.total_state_size)\n",
    "        return self._cell\n",
    "\n",
    "    @property\n",
    "    def base_cell(self):\n",
    "        \"\"\"Base cell without the output wrapper\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        \"\"\"size of output excluding joy gradient dimensions\n",
    "        \"\"\"\n",
    "        return self._output_size\n",
    "\n",
    "    @property\n",
    "    def total_output_size(self):\n",
    "        \"\"\"size of output of rnn including joy gradient dimensions\n",
    "        \"\"\"\n",
    "        return self.output_size + self.total_state_size\n",
    "\n",
    "    @property\n",
    "    def num_unroll(self):\n",
    "        return self._num_unroll\n",
    "\n",
    "    @property\n",
    "    def state_saver(self):\n",
    "        return self._state_saver\n",
    "\n",
    "    @property\n",
    "    def init_state(self):\n",
    "        if not self._init_state:\n",
    "            raise Exception(\"self._init_state needs to be defined during\"\n",
    "                            \" graph construction time, using the initial\"\n",
    "                            \" state from the state saving queue.\")\n",
    "        else:\n",
    "            return self._init_state\n",
    "\n",
    "    def build_joy_gradient_rnn(self, inputs, sequence_length):\n",
    "        with tf.name_scope('JNN'):\n",
    "            inputs = tf.unstack(inputs, num=self.num_unroll, axis=1)\n",
    "            outputs, final_state = tf.nn.static_state_saving_jnn(\n",
    "                cell=self.cell,\n",
    "                inputs=inputs,\n",
    "                state_saver=self.state_saver,\n",
    "                state_name=self.state_name,\n",
    "                sequence_length=sequence_length)\n",
    "\n",
    "            with tf.name_scope('joy_gradient'):\n",
    "                joy_gradient = tf.slice(\n",
    "                    outputs[0], begin=[0, self.output_size], size=[-1, -1])\n",
    "                joy_gradient = tf.split(\n",
    "                    joy_gradient, nest.flatten(self.state_size), axis=1)\n",
    "\n",
    "            with tf.name_scope('logits'):\n",
    "                stacked_outputs = tf.stack(outputs, axis=1)\n",
    "                logits = tf.slice(stacked_outputs, begin=[0, 0, 0], size=[-1, -1, self.output_size])\n",
    "\n",
    "        return logits, final_state, joy_gradient\n",
    "\n",
    "    def build_next_synthetic_gradient(self, final_state, next_inputs):\n",
    "        with tf.name_scope('next_synthetic_gradient'):\n",
    "            next_inputs = tf.unstack(next_inputs, num=self.num_unroll, axis=1)\n",
    "            next_output, _ = self.cell(next_inputs[0], final_state)\n",
    "            next_joy_gradient = tf.slice(\n",
    "                next_output, begin=[0, self.output_size], size=[-1, -1])\n",
    "            next_joy_gradient = tf.split(\n",
    "                next_joy_gradient, nest.flatten(self.state_size), axis=1)\n",
    "        return next_joy_gradient\n",
    "\n",
    "    @property\n",
    "    def zero_state(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          `list` of `Tensor` of [hidden_size] shape.\n",
    "        \"\"\"\n",
    "        init_states = self.cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "        init_states = nest.flatten(init_states)\n",
    "        init_states = tuple([tf.squeeze(state, axis=0) for state in init_states])\n",
    "        return init_states\n",
    "\n",
    "    @property\n",
    "    def init_state_dict(self):\n",
    "        return {k: v for k, v in zip(\n",
    "            nest.flatten(self.state_name), nest.flatten(self.zero_state))}\n",
    "\n",
    "    @property\n",
    "    def state_name(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          nested `tuple` of `str`\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "\n",
    "        def gen_state_name(zs):\n",
    "            nonlocal i\n",
    "            if isinstance(zs, tuple):\n",
    "                return tuple([gen_state_name(s) for s in zs])\n",
    "            else:\n",
    "                name = 'state_{}'.format(i)\n",
    "                i += 1\n",
    "                return name\n",
    "\n",
    "        return gen_state_name(self.state_size)\n",
    "\n",
    "    @property\n",
    "    def zero_initial_state_dict(self):\n",
    "        \"\"\"This property is used only for state saving queue\n",
    "        Returns:\n",
    "          `dict` where item is state_name:zero_state\n",
    "        \"\"\"\n",
    "        return {k: v for k, v in zip(\n",
    "            nest.flatten(self.state_name), nest.flatten(self.zero_state))}\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self.base_cell.state_size\n",
    "\n",
    "    @property\n",
    "    def total_state_size(self):\n",
    "        state_sizes = nest.flatten(self.base_cell.state_size)\n",
    "        return sum(state_sizes)\n",
    "\n",
    "    def gradient(self, loss, tvars, next_sg, final_state):\n",
    "        grad_local = tf.gradients(ys=loss, xs=tvars, grad_ys=None,\n",
    "                                  name='local_gradients')\n",
    "        received_sg = [tf.where(self.is_done, tf.zeros_like(nsg), nsg) for nsg in next_sg]\n",
    "        grad_sg = tf.gradients(\n",
    "            ys=nest.flatten(final_state), xs=tvars, grad_ys=received_sg,\n",
    "            name='synthetic_gradients')\n",
    "        grad = [tf.add(gl, gs) if gs is not None else gl for gl, gs in zip(grad_local, grad_sg)]\n",
    "        return grad\n",
    "\n",
    "    def sg_target(self, loss, next_sg, final_state):\n",
    "        local_grad = tf.gradients(ys=loss, xs=nest.flatten(self.init_state))\n",
    "        next_sg = [tf.where(self.is_done, tf.zeros_like(grad), grad) for grad in next_sg]\n",
    "        future_grad = tf.gradients(\n",
    "            ys=nest.flatten(final_state),\n",
    "            xs=nest.flatten(self.init_state),\n",
    "            grad_ys=next_sg)\n",
    "        # for two sequence, the target is bootstrapped\n",
    "        # at the end sequence, the target is only single sequence\n",
    "        sg_target = [tf.stop_gradient(tf.add(lg, fg))\n",
    "                     for lg, fg in zip(local_grad, future_grad)]\n",
    "        return sg_target\n",
    "\n",
    "    @property\n",
    "    def is_done(self):\n",
    "        return self._is_done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JoyReader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Py3 = sys.version_info[0] == 3\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        if Py3:\n",
    "            return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "        else:\n",
    "            return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "    data = _read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "    \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "    Reads PTB text files, converts strings to integer ids,\n",
    "    and performs mini-batching of the inputs.\n",
    "    Args:\n",
    "      data_path: string path to the directory where simple-examples.tgz has\n",
    "        been extracted.\n",
    "    Returns:\n",
    "      tuple (train_data, valid_data, test_data, vocabulary)\n",
    "      where each of the data objects can be passed to PTBIterator.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "    \"\"\"Iterate on the raw PTB data.\n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "    Args:\n",
    "      raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "      batch_size: int, the batch size.\n",
    "      num_steps: int, the number of unrolls.\n",
    "      name: the name of this operation (optional).\n",
    "    Returns:\n",
    "      A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "      of the tuple is the same data time-shifted to the right by one.\n",
    "    Raises:\n",
    "      tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "        data_len = tf.size(raw_data)\n",
    "        batch_len = data_len // batch_size\n",
    "        data = tf.reshape(raw_data[0: batch_size * batch_len],\n",
    "                          [batch_size, batch_len])\n",
    "\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "        assertion = tf.assert_positive(\n",
    "            epoch_size,\n",
    "            message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "        with tf.control_dependencies([assertion]):\n",
    "            epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "        i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "        x = tf.strided_slice(data, [0, i * num_steps],\n",
    "                             [batch_size, (i + 1) * num_steps])\n",
    "        x.set_shape([batch_size, num_steps])\n",
    "        y = tf.strided_slice(data, [0, i * num_steps + 1],\n",
    "                             [batch_size, (i + 1) * num_steps + 1])\n",
    "        y.set_shape([batch_size, num_steps])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def _circular_shift(x, step_size, axis):\n",
    "    with tf.name_scope(\"circular_shift\"):\n",
    "        size = tf.shape(x)[axis]\n",
    "        x0, x1 = tf.split(x, [step_size, size - step_size], axis=axis)\n",
    "        x = tf.concat([x1, x0], axis=axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pdb_state_saver(raw_data, batch_size, num_steps, init_states,\n",
    "                    num_unroll, num_threads=3, capacity=1000, allow_small_batch=False,\n",
    "                    epoch=1000, name=None):\n",
    "    data_len = len(raw_data)\n",
    "    with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "        raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "        n_seq = (data_len - 1) // num_steps\n",
    "        # need to make sure the num_step is multiple of num_unroll\n",
    "        raw_data_x = tf.reshape(raw_data[0: n_seq * num_steps],\n",
    "                                [n_seq, num_steps])\n",
    "        next_raw_data_x = _circular_shift(raw_data_x, num_unroll, axis=1)\n",
    "        raw_data_y = tf.reshape(raw_data[1: (n_seq * num_steps + 1)],\n",
    "                                [n_seq, num_steps])\n",
    "        next_raw_data_y = _circular_shift(raw_data_y, num_unroll, axis=1)\n",
    "\n",
    "        keys = tf.convert_to_tensor(\n",
    "            ['seq_{}'.format(i) for i in range(n_seq)], name=\"key\", dtype=tf.string)\n",
    "        seq_len = tf.tile([num_steps], [n_seq])\n",
    "        data = tf.data.Dataset.from_tensor_slices(\n",
    "            (keys, raw_data_x, next_raw_data_x, raw_data_y, next_raw_data_y, seq_len))\n",
    "        data = data.repeat(epoch)\n",
    "        iterator = data.make_one_shot_iterator()\n",
    "        next_key, next_x, next_next_x, next_y, next_next_y, next_len = iterator.get_next()\n",
    "        seq_dict = {'x': next_x, 'next_x': next_next_x, 'y': next_y, 'next_y': next_next_y}\n",
    "        batch = batch_sequences_with_states(\n",
    "            input_key=next_key,\n",
    "            input_sequences=seq_dict,\n",
    "            input_context={},\n",
    "            input_length=next_len,\n",
    "            initial_states=init_states,\n",
    "            num_unroll=num_unroll,\n",
    "            batch_size=batch_size,\n",
    "            num_threads=num_threads,\n",
    "            capacity=capacity,\n",
    "            allow_small_batch=allow_small_batch,\n",
    "            pad=True)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def seq_generator():\n",
    "    max_len = 11\n",
    "    num_feat = 3\n",
    "    seq_len = tf.cast(tf.ceil(tf.random.uniform() * max_len), dtype=tf.int32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tf.random.uniform([seq_len, num_feat]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JoyStep Algorithms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"model\", \"small\",\n",
    "    \"A type of model. Possible options are: small, medium, large.\")\n",
    "flags.DEFINE_string(\"data_path\", None,\n",
    "                    \"Where the training/test data is stored.\")\n",
    "flags.DEFINE_string(\"save_path\", '/tmp/sgrnn/',\n",
    "                    \"Model output directory.\")\n",
    "flags.DEFINE_bool(\"use_fp16\", False,\n",
    "                  \"Train using 16-bit floats instead of 32bit floats\")\n",
    "flags.DEFINE_integer(\"num_gpus\", 1,\n",
    "                     \"If larger than 1, Grappler AutoParallel optimizer \"\n",
    "                     \"will create multiple training replicas with each GPU \"\n",
    "                     \"running one replica.\")\n",
    "flags.DEFINE_string(\"rnn_mode\", None,\n",
    "                    \"The low level implementation of lstm cell: one of CUDNN, \"\n",
    "                    \"BASIC, and BLOCK, representing cudnn_lstm, basic_lstm, \"\n",
    "                    \"and lstm_block_cell classes.\")\n",
    "FLAGS = flags.FLAGS\n",
    "BASIC = \"basic\"\n",
    "CUDNN = \"cudnn\"\n",
    "BLOCK = \"block\"\n",
    "\n",
    "\n",
    "def _get_total_hidden_size(cell):\n",
    "    if isinstance(cell, tuple):\n",
    "        cells = nest.flatten(cell)\n",
    "        sizes = [c.get_shape().as_list()[0] for c in cells]\n",
    "\n",
    "\n",
    "def data_type():\n",
    "    return tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\n",
    "\n",
    "class PTBInput(object):\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(self, config, data, init_states, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        # self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.epoch_size = (len(data) - 1) // config.num_unroll // batch_size\n",
    "        self.num_unroll = num_unroll = config.num_unroll\n",
    "        self.output_size = config.vocab_size\n",
    "        batch = pdb_state_saver(\n",
    "            raw_data=data, batch_size=batch_size,\n",
    "            num_steps=num_steps, init_states=init_states,\n",
    "            num_unroll=num_unroll, num_threads=3, capacity=1000,\n",
    "            allow_small_batch=False, epoch=1000)\n",
    "        self.input_data = batch.sequences['x']\n",
    "        self.next_input_data = batch.sequences['next_x']\n",
    "        self.targets = batch.sequences['y']\n",
    "        self.next_targets = batch.sequences['next_y']\n",
    "        self.state_saver = batch\n",
    "        self.length = batch.length\n",
    "        self.sequence = batch.sequence\n",
    "        self.sequence_count = batch.sequence_count\n",
    "\n",
    "\n",
    "class PTBModel(SyntheticGradientRNN):\n",
    "    def __init__(self, config, is_training):\n",
    "        super().__init__()\n",
    "        self._config = config\n",
    "        self._num_layers = config.num_layers\n",
    "        self._is_training = is_training\n",
    "        self._output_size = config.vocab_size\n",
    "        self._input = None\n",
    "        self._batch_size = None\n",
    "        self._num_steps = None\n",
    "        self._sequence_length = None\n",
    "        self._is_done = None\n",
    "\n",
    "        self._rnn_params = None\n",
    "        self._final_state = None\n",
    "\n",
    "    @property\n",
    "    def is_training(self):\n",
    "        return self._is_training\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "\n",
    "    @property\n",
    "    def num_layers(self):\n",
    "        return self._num_layers\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def num_steps(self):\n",
    "        return self._num_steps\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self):\n",
    "        return self._sequence_length\n",
    "\n",
    "    @property\n",
    "    def is_done(self):\n",
    "        return self._is_done\n",
    "\n",
    "    @property\n",
    "    def base_cell(self):\n",
    "        if not self._base_cell:\n",
    "            self._base_cell = tf.contrib.rnn.MultiJNNCell(\n",
    "                [self._make_single_cell() for _ in range(self.num_layers)],\n",
    "                state_is_tuple=True)\n",
    "        return self._base_cell\n",
    "\n",
    "    def _make_single_cell(self):\n",
    "        cell = self._get_lstm_cell(self.config, self.is_training)\n",
    "        if self.is_training and self.config.keep_prob < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(\n",
    "                cell, output_keep_prob=self.config.keep_prob)\n",
    "        return cell\n",
    "\n",
    "    def _get_lstm_cell(self, config, is_training):\n",
    "        if config.rnn_mode == BASIC:\n",
    "            return tf.contrib.rnn.BasicLSTMCell(\n",
    "                config.hidden_size, forget_bias=0.0, state_is_tuple=True,\n",
    "                reuse=not is_training)\n",
    "        if config.rnn_mode == BLOCK:\n",
    "            return tf.contrib.rnn.LSTMBlockCell(\n",
    "                config.hidden_size, forget_bias=0.0)\n",
    "        raise ValueError(\"rnn_mode %s not supported\" % config.rnn_mode)\n",
    "\n",
    "    def build_graph(self, input_):\n",
    "        self._input = input_\n",
    "        self._batch_size = input_.batch_size\n",
    "        self._num_steps = input_.num_steps\n",
    "        self._num_unroll = input_.num_unroll\n",
    "        self._state_saver = input_.state_saver\n",
    "        self._init_state = [self.state_saver.state(name)\n",
    "                            for name in nest.flatten(self.state_name)]\n",
    "        self._sequence_length = input_.length\n",
    "        self._is_done = tf.equal(input_.sequence, input_.sequence_count - 1)\n",
    "        size = self.config.hidden_size\n",
    "        vocab_size = self.config.vocab_size\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, size], dtype=data_type())\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "            next_inputs = tf.nn.embedding_lookup(embedding, input_.next_input_data)\n",
    "\n",
    "        if self.is_training and self.config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, self.config.keep_prob)\n",
    "            next_inputs = tf.nn.dropout(next_inputs, self.config.keep_prob)\n",
    "\n",
    "        logits, final_state, sg = self.build_synthetic_gradient_jnn(\n",
    "            inputs, self.sequence_length)\n",
    "        next_sg = self.build_next_synthetic_gradient(final_state, next_inputs)\n",
    "\n",
    "        self._final_state = final_state\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            input_.targets,\n",
    "            tf.ones([self.batch_size, self.num_unroll], dtype=data_type()),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "        loss = tf.reduce_sum(loss, axis=0, keep_dims=False)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        grad = self.gradient(loss, tvars, next_sg, final_state)\n",
    "\n",
    "        sg_target = self.sg_target(loss, next_sg, final_state)\n",
    "        sg_loss = tf.losses.mean_squared_error(labels=tf.stack(sg_target), predictions=tf.stack(sg))\n",
    "        sg_grad = tf.gradients(ys=sg_loss, xs=tvars)\n",
    "\n",
    "        # Update the cost\n",
    "        self._cost = tf.reduce_sum(loss)\n",
    "        self._final_state = final_state\n",
    "        self._sg_cost = sg_loss\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        grads, _ = tf.clip_by_global_norm(grad,\n",
    "                                          self.config.max_grad_norm)\n",
    "        sg_grad, _ = tf.clip_by_global_norm(sg_grad,\n",
    "                                            self.config.max_grad_norm)\n",
    "        optimizer = tf.train.AdamOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "        optimizer_sg = tf.train.AdamOptimizer(self._lr)\n",
    "        self._train_sg_op = optimizer_sg.apply_gradients(\n",
    "            grads_and_vars=zip(sg_grad, tvars),\n",
    "            global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def train_sg_op(self):\n",
    "        return self._train_sg_op\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def sg_cost(self):\n",
    "        return self._sg_cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "\n",
    "class SmallConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4\n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "    num_unroll = 5\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "    \"\"\"Medium config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "    num_unroll = 5\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "    \"\"\"Large config.\"\"\"\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "    num_unroll = 5\n",
    "\n",
    "\n",
    "class TestConfig(object):\n",
    "    \"\"\"Tiny config, for testing.\"\"\"\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 30\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 8\n",
    "    vocab_size = 10000\n",
    "    rnn_mode = BLOCK\n",
    "    num_unroll = 3\n",
    "\n",
    "\n",
    "def run_epoch(session, model, global_step, train_ops=None,\n",
    "              summary_op=None, verbose=False, summary_writer=None):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "        \"sg_cost\": model.sg_cost\n",
    "    }\n",
    "\n",
    "    is_training = False\n",
    "    if train_ops is not None:\n",
    "        fetches.update(train_ops)\n",
    "        is_training = True\n",
    "\n",
    "    if summary_op is not None:\n",
    "        summary_dict = {'summary': summary_op}\n",
    "    else:\n",
    "        summary_dict = {}\n",
    "\n",
    "    fetches_w_summary = copy.copy(fetches)\n",
    "    fetches_w_summary.update(summary_dict)\n",
    "\n",
    "    for step in range(model.input.epoch_size):\n",
    "        if is_training:\n",
    "            global_step += 1\n",
    "        if step % 10 == 0:\n",
    "            vals = session.run(fetches_w_summary)\n",
    "            summary = vals['summary']\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "        else:\n",
    "            vals = session.run(fetches)\n",
    "\n",
    "        cost = vals[\"cost\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                  (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n",
    "                   iters * model.input.batch_size * max(1, FLAGS.num_gpus) /\n",
    "                   (time.time() - start_time)))\n",
    "\n",
    "    return np.exp(costs / iters), global_step\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"Get model config.\"\"\"\n",
    "    config = None\n",
    "    if FLAGS.model == \"small\":\n",
    "        config = SmallConfig()\n",
    "    elif FLAGS.model == \"medium\":\n",
    "        config = MediumConfig()\n",
    "    elif FLAGS.model == \"large\":\n",
    "        config = LargeConfig()\n",
    "    elif FLAGS.model == \"test\":\n",
    "        config = TestConfig()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model: %s\", FLAGS.model)\n",
    "    if FLAGS.rnn_mode:\n",
    "        config.rnn_mode = FLAGS.rnn_mode\n",
    "    if FLAGS.num_gpus != 1 or tf.__version__ < \"1.3.0\":\n",
    "        config.rnn_mode = BASIC\n",
    "    return config\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    if not FLAGS.data_path:\n",
    "        raise ValueError(\"Must set --data_path to PTB data directory\")\n",
    "\n",
    "    raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "    train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "    config = get_config()\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config)\n",
    "            train_input = PTBInput(\n",
    "                config=config, data=train_data, name=\"TrainInput\",\n",
    "                init_states=m.init_state_dict)\n",
    "            m.build_graph(train_input)\n",
    "        tf.summary.scalar(\"Training Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Synthetic Gradient MSE\", m.sg_cost)\n",
    "        tf.summary.scalar(\"Learning Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=config)\n",
    "            valid_input = PTBInput(\n",
    "                config=config, data=valid_data, name=\"ValidInput\",\n",
    "                init_states=mvalid.init_state_dict)\n",
    "            mvalid.build_graph(valid_input)\n",
    "        tf.summary.scalar(\"Validation Loss\", mvalid.cost)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=config)\n",
    "            test_input = PTBInput(\n",
    "                config=config, data=test_data,\n",
    "                init_states=mtest.init_state_dict, name=\"TestInput\")\n",
    "            mtest.build_graph(test_input)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.save_path + '/train',\n",
    "                                             session.graph)\n",
    "        valid_writer = tf.summary.FileWriter(FLAGS.save_path + '/valid')\n",
    "        print(\"begin training\")\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=session, coord=coord)\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        global_step = 0\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay = config.lr_decay ** max(i + 1. - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_perplexity, global_step = run_epoch(\n",
    "                session, m, global_step=global_step,\n",
    "                train_ops={\"train\": m.train_op, \"train_sg\": m.train_sg_op},\n",
    "                verbose=True, summary_op=summary_op, summary_writer=train_writer)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            valid_perplexity, global_step = run_epoch(session, mvalid, global_step=global_step,\n",
    "                                                      summary_op=summary_op, summary_writer=valid_writer)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity, global_step = run_epoch(session, mtest, global_step=global_step)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}